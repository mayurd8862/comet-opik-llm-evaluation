{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a type of artificial intelligence (AI) that allows computers to learn and improve their performance on a task without being explicitly programmed for it. In machine learning, the computer is trained on a dataset, which it uses to learn patterns and make predictions or decisions without being specifically programmed to perform the task.\\n\\nThere are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\\n\\n1. Supervised learning: In supervised learning, the computer is trained on a labeled dataset, where the input data and the correct output are both provided. The goal is to learn a mapping from inputs to outputs, so that the computer can make accurate predictions on new, unseen data.\\n2. Unsupervised learning: In unsupervised learning, the computer is trained on an unlabeled dataset, where only the input data is provided. The goal is to discover hidden patterns or structure in the data, such as grouping similar data points together or identifying outliers.\\n3. Reinforcement learning: In reinforcement learning, the computer learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. The goal is to learn a policy that maximizes the cumulative reward over time.\\n\\nMachine learning has many applications in areas such as computer vision, natural language processing, speech recognition, recommendation systems, and fraud detection, among others.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opik import track\n",
    "import os \n",
    "os.environ[\"OPIK_PROJECT_NAME\"] = \"Tracing_demo\"\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "# Create your chain\n",
    "@track\n",
    "def your_llm_application(input_text):\n",
    "    response = llm.invoke(input_text)\n",
    "    return response.content\n",
    "\n",
    "your_llm_application(\"Hey what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_task(x):\n",
    "    return {\n",
    "        \"output\": your_llm_application(x['input'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import Opik\n",
    "\n",
    "client = Opik()\n",
    "dataset = client.get_or_create_dataset(name=\"Example dataset\")\n",
    "\n",
    "dataset.insert([\n",
    "    # Simple greetings and common phrases\n",
    "    {\"input\": \"Good morning!\", \"expected_output\": \"Good morning!\"},\n",
    "    {\"input\": \"How are you?\", \"expected_output\": \"I'm fine, thank you! How about you?\"},\n",
    "    \n",
    "    # Trivia questions\n",
    "    {\"input\": \"What is the capital of Japan?\", \"expected_output\": \"Tokyo\"},\n",
    "    {\"input\": \"Who wrote 'Hamlet'?\", \"expected_output\": \"William Shakespeare\"},\n",
    "    \n",
    "    # Mathematical questions\n",
    "    {\"input\": \"What is 12 + 15?\", \"expected_output\": \"27\"},\n",
    "    {\"input\": \"What is the square root of 64?\", \"expected_output\": \"8\"},\n",
    "    \n",
    "    # Logic and reasoning\n",
    "    {\"input\": \"If all cats are mammals and some mammals are dogs, are all cats dogs?\", \"expected_output\": \"No, not all cats are dogs.\"},\n",
    "    {\"input\": \"What is the opposite of 'always'?\", \"expected_output\": \"Never\"},\n",
    "    \n",
    "    # Contextual understanding\n",
    "    {\"input\": \"Translate 'Hola' to English.\", \"expected_output\": \"Hello\"},\n",
    "    {\"input\": \"What does 'photosynthesis' mean?\", \"expected_output\": \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water.\"},\n",
    "    \n",
    "    # Coding-related questions\n",
    "    {\"input\": \"Write a Python function to reverse a string.\", \n",
    "     \"expected_output\": \"def reverse_string(s):\\n    return s[::-1]\"},\n",
    "    {\"input\": \"What is the time complexity of binary search?\", \"expected_output\": \"O(log n)\"},\n",
    "    \n",
    "    # General knowledge\n",
    "    {\"input\": \"What is the largest planet in our solar system?\", \"expected_output\": \"Jupiter\"},\n",
    "    {\"input\": \"Name a primary color.\", \"expected_output\": \"Red\"},\n",
    "    \n",
    "    # Situational responses\n",
    "    {\"input\": \"What should I wear in the rain?\", \"expected_output\": \"You should wear a waterproof jacket and carry an umbrella.\"},\n",
    "    {\"input\": \"How do I make a cup of tea?\", \n",
    "     \"expected_output\": \"Boil some water, add a tea bag to a cup, pour the hot water, let it steep for a few minutes, then remove the tea bag and enjoy.\"}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "\n",
    "hallucination_metric = Hallucination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/24 [00:00<?, ?it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation:   4%|▍         | 1/24 [00:01<00:23,  1.03s/it]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation:  21%|██        | 5/24 [00:01<00:04,  4.08it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation:  42%|████▏     | 10/24 [00:01<00:01,  7.44it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation:  67%|██████▋   | 16/24 [00:02<00:00, 13.21it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation: 100%|██████████| 24/24 [00:03<00:00,  7.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Example dataset (24 samples) ───────────────╮\n",
       "│                                              │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:04                  │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 24                        │\n",
       "│                                              │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: None (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 24 failed</span> │\n",
       "│                                              │\n",
       "╰──────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ Example dataset (24 samples) ───────────────╮\n",
       "│                                              │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:04                  │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 24                        │\n",
       "│                                              │\n",
       "│ \u001b[1;32mhallucination_metric: None (avg)\u001b[0m\u001b[31m - 24 failed\u001b[0m │\n",
       "│                                              │\n",
       "╰──────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/mayurd8862/experiments/0194582d-5557-76b4-93c4-9b0cb7e52297/compare?experiments=%5B%22019458a8-e98e-7da2-9127-6ef4f9ae3f60%22%5D\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=72935;https://www.comet.com/opik/mayurd8862/experiments/0194582d-5557-76b4-93c4-9b0cb7e52297/compare?experiments=%5B%22019458a8-e98e-7da2-9127-6ef4f9ae3f60%22%5D\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Hallucination\n",
    "\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "\n",
    "model = LiteLLMChatModel(\n",
    "    name=\"mixtral-8x7b-32768\"\n",
    ")\n",
    "\n",
    "hallucination_metric = Hallucination(model=model)\n",
    "\n",
    "evaluation = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[hallucination_metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "Evaluation:   8%|▊         | 2/24 [00:01<00:19,  1.15it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\u001b[AOPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 691, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 595, in completion\n",
      "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 361, in _get_openai_client\n",
      "    _new_client = OpenAI(\n",
      "                  ^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py\", line 110, in __init__\n",
      "    raise OpenAIError(\n",
      "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1602, in completion\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 1575, in completion\n",
      "    response = openai_chat_completions.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py\", line 701, in completion\n",
      "    raise OpenAIError(\n",
      "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\scorer.py\", line 37, in _score_test_case\n",
      "    result = metric.score(**score_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 173, in wrapper\n",
      "    raise exception\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py\", line 163, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py\", line 94, in score\n",
      "    model_output = self._model.generate_string(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 123, in generate_string\n",
      "    response = self.generate_provider_response(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py\", line 150, in generate_provider_response\n",
      "    response = self._engine.completion(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 1024, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py\", line 902, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py\", line 2957, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 2189, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py\", line 355, in exception_type\n",
      "    raise AuthenticationError(\n",
      "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Evaluation: 100%|██████████| 24/24 [00:03<00:00,  6.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ Example dataset (24 samples) ───────────────╮\n",
       "│                                              │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:04                  │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 24                        │\n",
       "│                                              │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: None (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 24 failed</span> │\n",
       "│                                              │\n",
       "╰──────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ Example dataset (24 samples) ───────────────╮\n",
       "│                                              │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:04                  │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 24                        │\n",
       "│                                              │\n",
       "│ \u001b[1;32mhallucination_metric: None (avg)\u001b[0m\u001b[31m - 24 failed\u001b[0m │\n",
       "│                                              │\n",
       "╰──────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/mayurd8862/experiments/0194582d-5557-76b4-93c4-9b0cb7e52297/compare?experiments=%5B%22019458ae-4667-7bea-a2d1-8cbf58f6d021%22%5D\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=594079;https://www.comet.com/opik/mayurd8862/experiments/0194582d-5557-76b4-93c4-9b0cb7e52297/compare?experiments=%5B%22019458ae-4667-7bea-a2d1-8cbf58f6d021%22%5D\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvaluationResult' object has no attribute 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m\n\u001b[0;32m     46\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m     47\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     48\u001b[0m     task\u001b[38;5;241m=\u001b[39mevaluation_task,\n\u001b[0;32m     49\u001b[0m     scoring_metrics\u001b[38;5;241m=\u001b[39m[hallucination_metric],\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EvaluationResult' object has no attribute 'results'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "from opik import Opik, track\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Equals, Hallucination\n",
    "from opik.integrations.openai import track_openai\n",
    "import openai\n",
    "\n",
    "# from opik import track\n",
    "import os \n",
    "\n",
    "# Set your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the model with proper Groq prefix\n",
    "model = LiteLLMChatModel(\n",
    "    name=\"groq/mixtral-8x7b-32768\",  # Note the 'groq/' prefix\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")  # Explicitly pass the API key\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OPIK_PROJECT_NAME\"] = \"Tracing_demo\"\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "# Create your chain\n",
    "@track\n",
    "def your_llm_application(input_text):\n",
    "    response = llm.invoke(input_text)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def evaluation_task(x):\n",
    "    return {\n",
    "        \"output\": your_llm_application(x['input'])\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the hallucination metric\n",
    "hallucination_metric = Hallucination(model=model)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation = evaluate(\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[hallucination_metric],\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(evaluation.results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:691\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    690\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:595\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    593\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m422\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax retries must be an int\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    594\u001b[0m     )\n\u001b[1;32m--> 595\u001b[0m openai_client: OpenAI \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_openai_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:361\u001b[0m, in \u001b[0;36mOpenAIChatCompletion._get_openai_client\u001b[1;34m(self, is_async, api_key, api_base, api_version, timeout, max_retries, organization, client)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 361\u001b[0m     _new_client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m## SAVE CACHE KEY\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\openai\\_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py:1602\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[0;32m   1597\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1598\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m   1599\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m   1600\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[0;32m   1601\u001b[0m     )\n\u001b[1;32m-> 1602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1605\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py:1575\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1575\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[0;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1595\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:701\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    700\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 701\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    702\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39merror_text, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[0;32m    703\u001b[0m )\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m hallucination_metric \u001b[38;5;241m=\u001b[39m Hallucination(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mgroq_model\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Use the metric\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mhallucination_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe capital of France is Paris. It is famous for its iconic Eiffel Tower and rich cultural heritage.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py:173\u001b[0m, in \u001b[0;36mBaseTrackDecorator._tracked_sync.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    166\u001b[0m         logging_messages\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED_FROM_TRACKED_FUNCTION,\n\u001b[0;32m    167\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    172\u001b[0m     error_info \u001b[38;5;241m=\u001b[39m error_info_collector\u001b[38;5;241m.\u001b[39mcollect(exception)\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     generator_or_generator_container \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generators_handler(\n\u001b[0;32m    176\u001b[0m         result,\n\u001b[0;32m    177\u001b[0m         track_options\u001b[38;5;241m.\u001b[39mcapture_output,\n\u001b[0;32m    178\u001b[0m         track_options\u001b[38;5;241m.\u001b[39mgenerations_aggregator,\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\decorator\\base_track_decorator.py:163\u001b[0m, in \u001b[0;36mBaseTrackDecorator._tracked_sync.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m error_info: Optional[ErrorInfoDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    165\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    166\u001b[0m         logging_messages\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED_FROM_TRACKED_FUNCTION,\n\u001b[0;32m    167\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\metrics\\llm_judges\\hallucination\\metric.py:94\u001b[0m, in \u001b[0;36mHallucination.score\u001b[1;34m(self, input, output, context, **ignored_kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03mCalculate the hallucination score for the given input, output, and optional context field.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m        is detected, 0.0 otherwise, along with the reason for the verdict.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m llm_query \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mgenerate_query(\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m     90\u001b[0m     output\u001b[38;5;241m=\u001b[39moutput,\n\u001b[0;32m     91\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m     92\u001b[0m     few_shot_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfew_shot_examples,\n\u001b[0;32m     93\u001b[0m )\n\u001b[1;32m---> 94\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHallucinationResponseFormat\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_model_output(model_output)\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py:123\u001b[0m, in \u001b[0;36mLiteLLMChatModel.generate_string\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m valid_litellm_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_supported_params(kwargs)\n\u001b[0;32m    116\u001b[0m request \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    117\u001b[0m     {\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    120\u001b[0m     },\n\u001b[0;32m    121\u001b[0m ]\n\u001b[1;32m--> 123\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_provider_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_litellm_params\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\opik\\evaluation\\models\\litellm_chat_model.py:150\u001b[0m, in \u001b[0;36mLiteLLMChatModel.generate_provider_response\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m valid_litellm_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_supported_params(kwargs)\n\u001b[0;32m    148\u001b[0m all_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completion_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalid_litellm_params}\n\u001b[1;32m--> 150\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py:1024\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1021\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1022\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1023\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\utils.py:902\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    900\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 902\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    903\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\main.py:2957\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2956\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 2957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2189\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m   2188\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[1;32mc:\\Users\\mayur\\Desktop\\Project\\comet opik LLM evaluation\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:355\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[0;32m    353\u001b[0m ):\n\u001b[0;32m    354\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\n\u001b[0;32m    356\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthenticationError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    357\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m    358\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    359\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    360\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMistral API raised a streaming error\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n\u001b[0;32m    363\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "\n",
    "\n",
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "\n",
    "# Configure LiteLLMChatModel for Groq\n",
    "groq_model = LiteLLMChatModel(\n",
    "    name=\"llama3-8b-8192\",  # Replace with the name of the Groq model\n",
    "    base_url=\"http://localhost:8080/v1/completions\"  # Replace with the Groq model's API endpoint\n",
    ")\n",
    "\n",
    "# Initialize the Hallucination metric with the Groq model\n",
    "hallucination_metric = Hallucination(\n",
    "    model=groq_model\n",
    ")\n",
    "\n",
    "# Use the metric\n",
    "score = hallucination_metric.score(\n",
    "    input=\"What is the capital of France?\",\n",
    "    output=\"The capital of France is Paris. It is famous for its iconic Eiffel Tower and rich cultural heritage.\"\n",
    ")\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(experiment_id='01945844-d030-7ab0-82fb-bc1705dbf27e', experiment_name='compatible_booby_1818', test_results=[TestResult(test_case=TestCase(trace_id='01945844-d6b8-7c0c-83bb-931a24ee90ec', dataset_item_id='0194582f-7839-736d-80f4-b290f57ef730', scoring_inputs={'input': 'Name a primary color.', 'expected_output': 'Red', 'output': \"The primary colors are red, blue, and yellow. In the additive color system, used in television and computer screens, the primary colors are red, green, and blue.\\n\\nIt's important to note that the primary colors can vary depending on the color model being used. The primary colors in the subtractive color system, used in printing, are cyan, magenta, and yellow. When these colors are combined, they produce a neutral or black color. In the additive color system, when red, green, and blue light are combined, they produce white light.\\n\\nI hope this helps! Let me know if you have any other questions.\"}, task_output={'output': \"The primary colors are red, blue, and yellow. In the additive color system, used in television and computer screens, the primary colors are red, green, and blue.\\n\\nIt's important to note that the primary colors can vary depending on the color model being used. The primary colors in the subtractive color system, used in printing, are cyan, magenta, and yellow. When these colors are combined, they produce a neutral or black color. In the additive color system, when red, green, and blue light are combined, they produce white light.\\n\\nI hope this helps! Let me know if you have any other questions.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6b2-7461-816c-849d52fa90e0', dataset_item_id='0194582f-783b-79b4-ac49-3f6d6b1bb0b0', scoring_inputs={'input': 'Tell me a joke.', 'expected_output': 'Why did the scarecrow win an award? Because he was outstanding in his field!', 'output': \"Sure, here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"}, task_output={'output': \"Sure, here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6a1-791d-9018-209e05706d7b', dataset_item_id='0194582f-783d-7cd7-8bc8-ce80c10cebf4', scoring_inputs={'input': 'What do you think about AI?', 'expected_output': 'AI is a fascinating field with great potential to solve complex problems and improve our lives.', 'output': 'I believe that AI has the potential to bring about significant benefits to society, including improvements in areas such as healthcare, education, and the economy. However, it is also important to acknowledge and address the potential risks and challenges associated with AI, such as issues related to privacy, security, fairness, and transparency. I advocate for the responsible development and use of AI, with a focus on ensuring that it is aligned with human values and serves to augment and empower human capabilities, rather than replacing or marginalizing them.'}, task_output={'output': 'I believe that AI has the potential to bring about significant benefits to society, including improvements in areas such as healthcare, education, and the economy. However, it is also important to acknowledge and address the potential risks and challenges associated with AI, such as issues related to privacy, security, fairness, and transparency. I advocate for the responsible development and use of AI, with a focus on ensuring that it is aligned with human values and serves to augment and empower human capabilities, rather than replacing or marginalizing them.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6c2-7a99-87ca-8ef8aa074915', dataset_item_id='0194582f-7838-7e05-8a48-63ce82a8c312', scoring_inputs={'input': 'What is the largest planet in our solar system?', 'expected_output': 'Jupiter', 'output': \"The largest planet in our solar system is Jupiter. It's known for its Great Red Spot, a storm that has been raging on the planet for at least 300 years. Jupiter is so large that over 1,300 Earths could fit inside of it!\"}, task_output={'output': \"The largest planet in our solar system is Jupiter. It's known for its Great Red Spot, a storm that has been raging on the planet for at least 300 years. Jupiter is so large that over 1,300 Earths could fit inside of it!\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d688-7902-89dc-b9b8f6d5fbff', dataset_item_id='01945844-cf36-7496-b063-be1ad7b0c376', scoring_inputs={'input': 'What is the capital of France?', 'output': 'The capital of France is Paris. Paris is one of the most populous cities in Europe, as well as one of the most visited cities in the world due to its rich history, art, culture, and fashion. It is home to many iconic landmarks, such as the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and the Palace of Versailles. The city is also known for its culinary scene and its role as a global center for business and finance.'}, task_output={'output': 'The capital of France is Paris. Paris is one of the most populous cities in Europe, as well as one of the most visited cities in the world due to its rich history, art, culture, and fashion. It is home to many iconic landmarks, such as the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and the Palace of Versailles. The city is also known for its culinary scene and its role as a global center for business and finance.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6b5-7e92-ba71-9e1d34157ec3', dataset_item_id='0194582f-783a-7b26-b95b-47f0657cad8f', scoring_inputs={'input': 'Write a haiku about nature.', 'expected_output': \"Whispering soft breeze\\nThrough the trees it gently weaves\\nNature's quiet song.\", 'output': \"Whispering leaves dance,\\nRivers sing their ancient tales,\\nNature's breath embraces.\"}, task_output={'output': \"Whispering leaves dance,\\nRivers sing their ancient tales,\\nNature's breath embraces.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6c5-7b26-b76c-e718a26ae25e', dataset_item_id='0194582f-7837-7af4-89ec-cad16b119a2a', scoring_inputs={'input': 'What is the time complexity of binary search?', 'expected_output': 'O(log n)', 'output': 'The time complexity of binary search is O(log n), where n is the number of elements in the sorted array being searched. This is because with each comparison, binary search effectively halves the number of elements it needs to search through, leading to a logarithmic time complexity.'}, task_output={'output': 'The time complexity of binary search is O(log n), where n is the number of elements in the sorted array being searched. This is because with each comparison, binary search effectively halves the number of elements it needs to search through, leading to a logarithmic time complexity.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6d5-76a3-993b-966d826c7d81', dataset_item_id='0194582f-7833-747e-98e0-f2c88b02ca79', scoring_inputs={'input': \"What is the opposite of 'always'?\", 'expected_output': 'Never', 'output': \"The opposite of 'always' is 'never'. 'Always' denotes something that consistently happens or is true under all conditions, while 'never' indicates that something never occurs or is not true at any time.\"}, task_output={'output': \"The opposite of 'always' is 'never'. 'Always' denotes something that consistently happens or is true under all conditions, while 'never' indicates that something never occurs or is not true at any time.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6a4-775f-8f5a-dea657b41f8e', dataset_item_id='0194582f-783c-77f6-937c-bdef46a4f319', scoring_inputs={'input': 'What is the best programming language?', 'expected_output': 'The best programming language depends on the use case. For web development, JavaScript is popular. For data science, Python is widely used.', 'output': 'The best programming language can vary depending on the specific task or project you are working on, as well as your personal preferences and experience. Some popular and widely-used programming languages include:\\n\\n* Python: a high-level, general-purpose language known for its simplicity and readability. It is often used for web development, data analysis, machine learning, and scripting.\\n* JavaScript: a popular language for web development, used for creating interactive elements on websites. It is the primary language for front-end development and is also used for server-side (back-end) development with technologies like Node.js.\\n* Java: a general-purpose, object-oriented language that is widely used in enterprise environments for building large-scale applications. It is known for its stability and scalability.\\n* C++: a high-performance, general-purpose language that is commonly used for systems programming, game development, and other performance-critical applications.\\n* Swift: a language developed by Apple for iOS, macOS, watchOS, and tvOS development. It is known for its safety and ease of use.\\n\\nUltimately, the best programming language for you will depend on your specific needs and goals. It may be helpful to learn multiple languages and choose the one that is most suitable for a particular project.'}, task_output={'output': 'The best programming language can vary depending on the specific task or project you are working on, as well as your personal preferences and experience. Some popular and widely-used programming languages include:\\n\\n* Python: a high-level, general-purpose language known for its simplicity and readability. It is often used for web development, data analysis, machine learning, and scripting.\\n* JavaScript: a popular language for web development, used for creating interactive elements on websites. It is the primary language for front-end development and is also used for server-side (back-end) development with technologies like Node.js.\\n* Java: a general-purpose, object-oriented language that is widely used in enterprise environments for building large-scale applications. It is known for its stability and scalability.\\n* C++: a high-performance, general-purpose language that is commonly used for systems programming, game development, and other performance-critical applications.\\n* Swift: a language developed by Apple for iOS, macOS, watchOS, and tvOS development. It is known for its safety and ease of use.\\n\\nUltimately, the best programming language for you will depend on your specific needs and goals. It may be helpful to learn multiple languages and choose the one that is most suitable for a particular project.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6d8-7155-b960-912ebc60b2f0', dataset_item_id='0194582f-7832-775f-87a4-95394c2dee48', scoring_inputs={'input': 'If all cats are mammals and some mammals are dogs, are all cats dogs?', 'expected_output': 'No, not all cats are dogs.', 'output': \"No, all cats are not dogs.\\n\\nHere's the logic behind this:\\n\\n1. We know that all cats are mammals.\\n2. We also know that some mammals are dogs.\\n3. However, this does not mean that all mammals are dogs, let alone all cats.\\n4. There are many other types of mammals, such as bears, lions, and whales, which are not dogs.\\n5. Cats belong to a different group within the mammal class and do not share the same characteristics as dogs.\\n6. Therefore, it is incorrect to say that all cats are dogs.\"}, task_output={'output': \"No, all cats are not dogs.\\n\\nHere's the logic behind this:\\n\\n1. We know that all cats are mammals.\\n2. We also know that some mammals are dogs.\\n3. However, this does not mean that all mammals are dogs, let alone all cats.\\n4. There are many other types of mammals, such as bears, lions, and whales, which are not dogs.\\n5. Cats belong to a different group within the mammal class and do not share the same characteristics as dogs.\\n6. Therefore, it is incorrect to say that all cats are dogs.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6d0-759b-a294-7e861604254f', dataset_item_id='0194582f-7834-715b-a262-75e4e60c60dd', scoring_inputs={'input': \"Translate 'Hola' to English.\", 'expected_output': 'Hello', 'output': 'The term \\'Hola\\' is actually already in English, as it is directly taken from the Spanish language. However, it is commonly used in English-speaking contexts as an informal greeting, equivalent to \"Hello.\" So, if you\\'re looking for the English translation of \\'Hola\\', it\\'s simply \\'Hello\\'.'}, task_output={'output': 'The term \\'Hola\\' is actually already in English, as it is directly taken from the Spanish language. However, it is commonly used in English-speaking contexts as an informal greeting, equivalent to \"Hello.\" So, if you\\'re looking for the English translation of \\'Hola\\', it\\'s simply \\'Hello\\'.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d685-7080-bd0b-f55c31c15499', dataset_item_id='01945844-cf37-7ddd-b520-a609cc243c04', scoring_inputs={'input': 'What is the capital of Germany?', 'output': \"The capital of Germany is Berlin. Established during the 13th century, Berlin grew to become the capital of the Margraviate of Brandenburg and later the Kingdom of Prussia, and the German Empire. It is now the capital of the Federal Republic of Germany. Located in northeastern Germany, Berlin is a major center for politics, culture, media, science, and the arts in Europe. The city is well known for its diverse architecture, vibrant culture, extensive history, and high quality of living. It's also home to the famous Brandenburg Gate and the Reichstag building.\"}, task_output={'output': \"The capital of Germany is Berlin. Established during the 13th century, Berlin grew to become the capital of the Margraviate of Brandenburg and later the Kingdom of Prussia, and the German Empire. It is now the capital of the Federal Republic of Germany. Located in northeastern Germany, Berlin is a major center for politics, culture, media, science, and the arts in Europe. The city is well known for its diverse architecture, vibrant culture, extensive history, and high quality of living. It's also home to the famous Brandenburg Gate and the Reichstag building.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6c8-750d-b657-3e4b772983c1', dataset_item_id='0194582f-7836-79c9-b049-0b54b9bbbd04', scoring_inputs={'input': 'Write a Python function to reverse a string.', 'expected_output': 'def reverse_string(s):\\n    return s[::-1]', 'output': 'Here is a simple Python function that reverses a string:\\n\\n```python\\ndef reverse_string(input_string):\\n    return input_string[::-1]\\n```\\n\\nThis function uses slicing to reverse the input string. The `[::-1]` slice means \"start at the end of the string and end at position 0, move with the step -1\" which effectively reverses the string.\\n\\nYou can use this function like this:\\n\\n```python\\nprint(reverse_string(\"Hello, World!\"))  # Output: !dlroW ,olleH\\n```'}, task_output={'output': 'Here is a simple Python function that reverses a string:\\n\\n```python\\ndef reverse_string(input_string):\\n    return input_string[::-1]\\n```\\n\\nThis function uses slicing to reverse the input string. The `[::-1]` slice means \"start at the end of the string and end at position 0, move with the step -1\" which effectively reverses the string.\\n\\nYou can use this function like this:\\n\\n```python\\nprint(reverse_string(\"Hello, World!\"))  # Output: !dlroW ,olleH\\n```'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d691-7cdc-900d-a7b7bf50cdb4', dataset_item_id='0194582f-783f-7106-adc6-49a7b646162c', scoring_inputs={'input': 'How do I make a cup of tea?', 'expected_output': 'Boil some water, add a tea bag to a cup, pour the hot water, let it steep for a few minutes, then remove the tea bag and enjoy.', 'output': \"Sure, I'd be happy to guide you through the process of making a cup of tea! Here are the steps:\\n\\n1. Boil water: The first step is to boil water. The amount of water you need will depend on the size of your teacup or mug. A good rule of thumb is to use one cup of water for every teaspoon of tea leaves.\\n2. Warm the teapot: While the water is boiling, warm your teapot by rinsing it out with hot water. This helps to keep the tea hot for longer.\\n3. Measure the tea: Next, measure out the amount of tea leaves you want to use. A typical amount is one teaspoon of tea leaves per cup of water, but this can vary depending on the type of tea and your personal preference.\\n4. Add the tea to the teapot: Place the tea leaves in a tea infuser or teabag, and then add it to the teapot.\\n5. Pour the water over the tea: Once the water has reached the correct temperature (see below), pour it over the tea leaves in the teapot.\\n6. Steep the tea: Let the tea steep for the recommended time. The steeping time will vary depending on the type of tea, but a general guideline is:\\n* Black tea: 3-5 minutes\\n* Green tea: 2-3 minutes\\n* White tea: 1-3 minutes\\n* Oolong tea: 3-5 minutes\\n* Herbal tea: 5-7 minutes\\n7. Remove the tea leaves: After the tea has steeped for the recommended time, remove the tea leaves by lifting out the infuser or discarding the teabag.\\n8. Serve the tea: Pour the tea into your teacup or mug, and enjoy!\\n\\nNote: The temperature of the water you use to make tea can affect the flavor. Here are some general guidelines:\\n\\n* Black tea: 200-212°F (93-100°C)\\n* Green tea: 160-180°F (71-82°C)\\n* White tea: 160-185°F (71-85°C)\\n* Oolong tea: 185-205°F (85-96°C)\\n* Herbal tea: 212°F (100°C)\\n\\nYou can use a kitchen thermometer to measure the temperature of the water, or you can estimate it based on how long it has been since the water boiled. For example, if you let the water sit for a few minutes after it boils, it will cool down to around 175-185°F (80-85°C), which is a good temperature for green or white tea.\"}, task_output={'output': \"Sure, I'd be happy to guide you through the process of making a cup of tea! Here are the steps:\\n\\n1. Boil water: The first step is to boil water. The amount of water you need will depend on the size of your teacup or mug. A good rule of thumb is to use one cup of water for every teaspoon of tea leaves.\\n2. Warm the teapot: While the water is boiling, warm your teapot by rinsing it out with hot water. This helps to keep the tea hot for longer.\\n3. Measure the tea: Next, measure out the amount of tea leaves you want to use. A typical amount is one teaspoon of tea leaves per cup of water, but this can vary depending on the type of tea and your personal preference.\\n4. Add the tea to the teapot: Place the tea leaves in a tea infuser or teabag, and then add it to the teapot.\\n5. Pour the water over the tea: Once the water has reached the correct temperature (see below), pour it over the tea leaves in the teapot.\\n6. Steep the tea: Let the tea steep for the recommended time. The steeping time will vary depending on the type of tea, but a general guideline is:\\n* Black tea: 3-5 minutes\\n* Green tea: 2-3 minutes\\n* White tea: 1-3 minutes\\n* Oolong tea: 3-5 minutes\\n* Herbal tea: 5-7 minutes\\n7. Remove the tea leaves: After the tea has steeped for the recommended time, remove the tea leaves by lifting out the infuser or discarding the teabag.\\n8. Serve the tea: Pour the tea into your teacup or mug, and enjoy!\\n\\nNote: The temperature of the water you use to make tea can affect the flavor. Here are some general guidelines:\\n\\n* Black tea: 200-212°F (93-100°C)\\n* Green tea: 160-180°F (71-82°C)\\n* White tea: 160-185°F (71-85°C)\\n* Oolong tea: 185-205°F (85-96°C)\\n* Herbal tea: 212°F (100°C)\\n\\nYou can use a kitchen thermometer to measure the temperature of the water, or you can estimate it based on how long it has been since the water boiled. For example, if you let the water sit for a few minutes after it boils, it will cool down to around 175-185°F (80-85°C), which is a good temperature for green or white tea.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d694-7042-9aba-f9978bd1e696', dataset_item_id='0194582f-783e-7dc6-8781-f5fa91912539', scoring_inputs={'input': 'What should I wear in the rain?', 'expected_output': 'You should wear a waterproof jacket and carry an umbrella.', 'output': \"When it's raining, it's important to wear clothing that will keep you dry and warm. Here are some suggestions:\\n\\n1. Waterproof jacket: A jacket with a waterproof coating will help keep you dry in the rain. Look for one with a hood to protect your head and neck.\\n2. Waterproof pants: If you'll be outside for a while, consider wearing waterproof pants to keep your legs dry. Some rain pants are designed to be worn over your regular pants, while others are designed to be worn as outerwear.\\n3. Waterproof shoes or boots: Wearing waterproof shoes or boots will help keep your feet dry. Look for shoes with a good tread to prevent slipping on wet surfaces.\\n4. Socks: Wear socks that will keep your feet warm and dry. Consider wearing wool socks or socks made of a moisture-wicking material.\\n5. Hat: A hat with a brim can help keep the rain off your face and hair.\\n6. Gloves: If it's cold outside, consider wearing gloves to keep your hands warm. Look for gloves that are waterproof or water-resistant.\\n7. Umbrella: An umbrella can help keep you dry, especially if you'll be walking in the rain. Look for an umbrella with a sturdy handle and a canopy that is large enough to cover you and your bag or backpack.\\n\\nRemember to avoid wearing cotton clothing, as it tends to hold moisture and can make you feel cold and clammy. Instead, opt for clothing made of materials like polyester, nylon, or Gore-Tex, which are designed to be water-resistant or waterproof.\"}, task_output={'output': \"When it's raining, it's important to wear clothing that will keep you dry and warm. Here are some suggestions:\\n\\n1. Waterproof jacket: A jacket with a waterproof coating will help keep you dry in the rain. Look for one with a hood to protect your head and neck.\\n2. Waterproof pants: If you'll be outside for a while, consider wearing waterproof pants to keep your legs dry. Some rain pants are designed to be worn over your regular pants, while others are designed to be worn as outerwear.\\n3. Waterproof shoes or boots: Wearing waterproof shoes or boots will help keep your feet dry. Look for shoes with a good tread to prevent slipping on wet surfaces.\\n4. Socks: Wear socks that will keep your feet warm and dry. Consider wearing wool socks or socks made of a moisture-wicking material.\\n5. Hat: A hat with a brim can help keep the rain off your face and hair.\\n6. Gloves: If it's cold outside, consider wearing gloves to keep your hands warm. Look for gloves that are waterproof or water-resistant.\\n7. Umbrella: An umbrella can help keep you dry, especially if you'll be walking in the rain. Look for an umbrella with a sturdy handle and a canopy that is large enough to cover you and your bag or backpack.\\n\\nRemember to avoid wearing cotton clothing, as it tends to hold moisture and can make you feel cold and clammy. Instead, opt for clothing made of materials like polyester, nylon, or Gore-Tex, which are designed to be water-resistant or waterproof.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-d6cb-7944-8484-af8a57f17cca', dataset_item_id='0194582f-7835-7a65-9c5f-ac8ce9a7cacf', scoring_inputs={'input': \"What does 'photosynthesis' mean?\", 'expected_output': 'Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water.', 'output': \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy, usually from the sun, into chemical energy in the form of glucose or other sugars. This process takes place in the chloroplasts of plant cells, primarily in leaves, using water and carbon dioxide as the raw materials. During photosynthesis, oxygen is also produced as a byproduct, which is released into the atmosphere. The overall chemical equation for photosynthesis is:\\n\\n6CO2 + 6H2O + light energy -> C6H12O6 + 6O2\\n\\nThis means that six molecules of carbon dioxide (CO2) react with six molecules of water (H2O) in the presence of light energy to produce one molecule of glucose (C6H12O6) and six molecules of oxygen (O2). Photosynthesis is essential for life on Earth, as it provides the oxygen that most organisms need to survive and is the primary source of energy for the planet's food chain.\"}, task_output={'output': \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy, usually from the sun, into chemical energy in the form of glucose or other sugars. This process takes place in the chloroplasts of plant cells, primarily in leaves, using water and carbon dioxide as the raw materials. During photosynthesis, oxygen is also produced as a byproduct, which is released into the atmosphere. The overall chemical equation for photosynthesis is:\\n\\n6CO2 + 6H2O + light energy -> C6H12O6 + 6O2\\n\\nThis means that six molecules of carbon dioxide (CO2) react with six molecules of water (H2O) in the presence of light energy to produce one molecule of glucose (C6H12O6) and six molecules of oxygen (O2). Photosynthesis is essential for life on Earth, as it provides the oxygen that most organisms need to survive and is the primary source of energy for the planet's food chain.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-db51-79e4-a05e-68b460b31395', dataset_item_id='0194582f-7831-7c08-a548-0619f4908464', scoring_inputs={'input': 'What is the square root of 64?', 'expected_output': '8', 'output': \"The square root of 64 is 8.\\n\\nHere's the reasoning:\\n\\n1. When you multiply a number by itself, it gives you the square of that number. For example, 4 × 4 = 16, so the square root of 16 is 4.\\n2. In this case, we need to find a number that, when multiplied by itself, equals 64.\\n3. We know that 8 × 8 = 64, so the square root of 64 is 8.\\n\\nYou can also use a calculator to find the square root of any number.\"}, task_output={'output': \"The square root of 64 is 8.\\n\\nHere's the reasoning:\\n\\n1. When you multiply a number by itself, it gives you the square of that number. For example, 4 × 4 = 16, so the square root of 16 is 4.\\n2. In this case, we need to find a number that, when multiplied by itself, equals 64.\\n3. We know that 8 × 8 = 64, so the square root of 64 is 8.\\n\\nYou can also use a calculator to find the square root of any number.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dca2-7c37-a38c-2625011e5c20', dataset_item_id='0194582f-782e-79f0-aa9b-41e2655bb724', scoring_inputs={'input': 'What is the capital of Japan?', 'expected_output': 'Tokyo', 'output': 'The capital of Japan is Tokyo. Established in 1869, Tokyo is one of the 47 prefectures of Japan. It is located on the eastern coast of the island of Honshu and includes the Izu and Ogasawara Islands. Tokyo is the center of the Greater Tokyo Area, which is the most populous metropolitan area in the world. It is known for its bustling city life, rich culture, and advanced technology.'}, task_output={'output': 'The capital of Japan is Tokyo. Established in 1869, Tokyo is one of the 47 prefectures of Japan. It is located on the eastern coast of the island of Honshu and includes the Izu and Ogasawara Islands. Tokyo is the center of the Greater Tokyo Area, which is the most populous metropolitan area in the world. It is known for its bustling city life, rich culture, and advanced technology.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dcb5-7558-a56a-1bfb7f0105c3', dataset_item_id='0194582f-782d-7240-8cda-ce1851266555', scoring_inputs={'input': 'How are you?', 'expected_output': \"I'm fine, thank you! How about you?\", 'output': 'I\\'m just a computer program, so I don\\'t have feelings or physical sensations. I don\\'t have the ability to be \"well\" or \"unwell.\" I\\'m here to help answer your questions to the best of my ability. Is there something you would like to know?'}, task_output={'output': 'I\\'m just a computer program, so I don\\'t have feelings or physical sensations. I don\\'t have the ability to be \"well\" or \"unwell.\" I\\'m here to help answer your questions to the best of my ability. Is there something you would like to know?'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dd05-7d77-b9a7-77932ff226af', dataset_item_id='0194582d-533f-78f7-9184-50b290c76c81', scoring_inputs={'input': 'Hello, world!', 'expected_output': 'Hello, world!', 'output': \"Hello! It's nice to meet you. Is there something specific you would like to talk about or learn today? I'm here to help with any questions you have about a wide variety of topics, including but not limited to: computer programming, technology, math, science, and engineering. Just let me know how I can assist you.\"}, task_output={'output': \"Hello! It's nice to meet you. Is there something specific you would like to talk about or learn today? I'm here to help with any questions you have about a wide variety of topics, including but not limited to: computer programming, technology, math, science, and engineering. Just let me know how I can assist you.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dcb8-7854-bc2d-629888490975', dataset_item_id='0194582f-782c-7b26-b0d4-1f86bc374e0d', scoring_inputs={'input': 'Good morning!', 'expected_output': 'Good morning!', 'output': \"Good morning! I'm here to help you with any questions you have about writing or grammar. What can I assist you with today?\"}, task_output={'output': \"Good morning! I'm here to help you with any questions you have about writing or grammar. What can I assist you with today?\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dd02-70e9-9f2e-cb0ffcb91f5c', dataset_item_id='0194582d-5340-7d80-a690-e3abd87e43a7', scoring_inputs={'input': 'What is the capital of France?', 'expected_output': 'Paris', 'output': 'The capital of France is Paris. Paris is one of the most populous cities in Europe, as well as one of the most visited cities in the world. It is known for its beautiful and historic architecture, such as the Eiffel Tower and Notre-Dame Cathedral, as well as its influential role in art, fashion, gastronomy, and culture. Paris is also home to many important cultural institutions, including the Louvre Museum, which houses one of the largest and most famous art collections in the world.'}, task_output={'output': 'The capital of France is Paris. Paris is one of the most populous cities in Europe, as well as one of the most visited cities in the world. It is known for its beautiful and historic architecture, such as the Eiffel Tower and Notre-Dame Cathedral, as well as its influential role in art, fashion, gastronomy, and culture. Paris is also home to many important cultural institutions, including the Louvre Museum, which houses one of the largest and most famous art collections in the world.'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dc9f-744b-b01c-0568891c9eee', dataset_item_id='0194582f-782f-7886-8c29-e206f67ae628', scoring_inputs={'input': \"Who wrote 'Hamlet'?\", 'expected_output': 'William Shakespeare', 'output': \"The tragedy 'Hamlet, Prince of Denmark' is a masterpiece of English literature, and it was written by none other than the legendary playwright, William Shakespeare. Shakespeare, often regarded as the greatest writer in the English language, created this profoundly influential work during the late Elizabethan or early Jacobean period around the start of the 17th century.\\n\\n'Hamlet' delves deep into the complexities of human nature, showcasing the tormented journey of its eponymous character as he grapples with betrayal, loss, love, and the challenging quest for revenge. This captivating story has left an indelible mark on the world of literature and continues to enthrall audiences through adaptations on stage, screen, and in various art forms.\"}, task_output={'output': \"The tragedy 'Hamlet, Prince of Denmark' is a masterpiece of English literature, and it was written by none other than the legendary playwright, William Shakespeare. Shakespeare, often regarded as the greatest writer in the English language, created this profoundly influential work during the late Elizabethan or early Jacobean period around the start of the 17th century.\\n\\n'Hamlet' delves deep into the complexities of human nature, showcasing the tormented journey of its eponymous character as he grapples with betrayal, loss, love, and the challenging quest for revenge. This captivating story has left an indelible mark on the world of literature and continues to enthrall audiences through adaptations on stage, screen, and in various art forms.\"}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)]), TestResult(test_case=TestCase(trace_id='01945844-dc33-7990-af55-6ae8f0a2d6c8', dataset_item_id='0194582f-7830-7586-b3b2-04087dda3479', scoring_inputs={'input': 'What is 12 + 15?', 'expected_output': '27', 'output': 'The sum of 12 and 15 is 27.\\n\\nHere\\'s how you can find the answer:\\n\\n1. Write down the two numbers you want to add, keeping them in the correct order.\\n   In this case, you want to find the sum of 12 and 15.\\n\\n2. Starting from the rightmost digit in each number, add the digits together.\\n   For this example, add 2 (from 12) and 5 (from 15). The result is 7.\\n\\n3. If the sum of the digits is greater than 10, carry over the \"extra\" amount to the next digit on the left.\\n   In this case, there is no need to carry over anything because 7 is less than 10.\\n\\n4. Move one digit to the left and repeat steps 2 and 3 until you have added all the digits.\\n   In this case, add 1 (from 12) and 1 (from 15), then add the carried over 0 from the previous step.\\n   The result is 2.\\n\\n5. Write down the final sum, which is 27 (2 from the last step and 7 from the first step).'}, task_output={'output': 'The sum of 12 and 15 is 27.\\n\\nHere\\'s how you can find the answer:\\n\\n1. Write down the two numbers you want to add, keeping them in the correct order.\\n   In this case, you want to find the sum of 12 and 15.\\n\\n2. Starting from the rightmost digit in each number, add the digits together.\\n   For this example, add 2 (from 12) and 5 (from 15). The result is 7.\\n\\n3. If the sum of the digits is greater than 10, carry over the \"extra\" amount to the next digit on the left.\\n   In this case, there is no need to carry over anything because 7 is less than 10.\\n\\n4. Move one digit to the left and repeat steps 2 and 3 until you have added all the digits.\\n   In this case, add 1 (from 12) and 1 (from 15), then add the carried over 0 from the previous step.\\n   The result is 2.\\n\\n5. Write down the final sum, which is 27 (2 from the last step and 7 from the first step).'}), score_results=[ScoreResult(name='hallucination_metric', value=0.0, reason='litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable', metadata=None, scoring_failed=True)])])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class GroqModel with abstract method agenerate_provider_response",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopik\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hallucination\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the custom Groq model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m groq_model \u001b[38;5;241m=\u001b[39m \u001b[43mGroqModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixtral-8x7b-32768\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or any other Groq model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use it with the metric\u001b[39;00m\n\u001b[0;32m      7\u001b[0m hallucination_metric \u001b[38;5;241m=\u001b[39m Hallucination(model\u001b[38;5;241m=\u001b[39mgroq_model)\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class GroqModel with abstract method agenerate_provider_response"
     ]
    }
   ],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "\n",
    "# Initialize the custom Groq model\n",
    "groq_model = GroqModel(model_name=\"mixtral-8x7b-32768\")  # or any other Groq model\n",
    "\n",
    "# Use it with the metric\n",
    "hallucination_metric = Hallucination(model=groq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation.models import LiteLLMChatModel\n",
    "\n",
    "model = LiteLLMChatModel(\n",
    "    name=\"groq/mixtral-8x7b-32768\"\n",
    ")\n",
    "\n",
    "hallucination_metric = Hallucination(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
